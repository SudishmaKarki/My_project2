{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Working directory: /Users/sudishmakarki/My_project2\n"
     ]
    }
   ],
   "source": [
    "# Set working directory (optional during development)\n",
    "import os\n",
    "os.chdir('/Users/sudishmakarki/My_project2')  # only if needed\n",
    "print(\" Working directory:\", os.getcwd())\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Custom Functions\n",
    "from models.data_interpolation import (\n",
    "    load_data,\n",
    "    preprocess_data,\n",
    "    split_train_test,\n",
    "    generate_time_series_splits\n",
    ")\n",
    "\n",
    "from models.model_sarimax import (\n",
    "    prepare_sarimax_data,\n",
    "    check_stationarity,\n",
    "    plot_acf_pacf,\n",
    "    fit_sarimax_model,\n",
    "    analyze_residual_spike,\n",
    "    ljung_box_test,\n",
    "    forecast_sarimax_model,\n",
    "    identify_peak_hours_sarimax,\n",
    "    evaluate_sarimax_metrics,\n",
    "    rolling_forecast_sarimax,\n",
    "    generate_future_forecast_sarimax,\n",
    "    group_forecast_by_hour,\n",
    "    \n",
    ")\n",
    "\n",
    "from models.model_sarimax import (\n",
    "    create_exogenous_variables,\n",
    "    fit_sarimax_with_exog,\n",
    "    ljung_box_test_refined_sarimax,\n",
    "    analyze_largest_residual_sarimax_exog,\n",
    "    forecast_with_exog,\n",
    "    analyze_peak_hours_exog,\n",
    "    evaluate_sarimax_exog_metrics,\n",
    "    rolling_forecast_sarimax_exog,\n",
    "    generate_future_forecast_sarimax_exog,\n",
    "    group_forecast_by_hour_sarimax_exog\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Functions\n",
    "from models.data_interpolation import (\n",
    "    load_data,\n",
    "    preprocess_data,\n",
    "    split_train_test,\n",
    "    generate_time_series_splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Year  Month  Season  DayOfWeek WeekDay  Hour  \\\n",
      "Timestamp                                                           \n",
      "2018-01-01 00:00:00  2018      1  Winter          1  Monday     0   \n",
      "2018-01-01 01:00:00  2018      1  Winter          1  Monday     1   \n",
      "2018-01-01 02:00:00  2018      1  Winter          1  Monday     2   \n",
      "2018-01-01 03:00:00  2018      1  Winter          1  Monday     3   \n",
      "2018-01-01 04:00:00  2018      1  Winter          1  Monday     4   \n",
      "\n",
      "                            Holiday Weather SpecialEvent  CustomerCount  \\\n",
      "Timestamp                                                                 \n",
      "2018-01-01 00:00:00  New Year's Day   Rainy          NaN              6   \n",
      "2018-01-01 01:00:00  New Year's Day   Windy          NaN             11   \n",
      "2018-01-01 02:00:00  New Year's Day   Snowy          NaN              9   \n",
      "2018-01-01 03:00:00  New Year's Day   Rainy          NaN             10   \n",
      "2018-01-01 04:00:00  New Year's Day  Cloudy          NaN             14   \n",
      "\n",
      "                     Orders  Revenue  StaffingLevel  \n",
      "Timestamp                                            \n",
      "2018-01-01 00:00:00       5    52.30              5  \n",
      "2018-01-01 01:00:00       7   129.41              7  \n",
      "2018-01-01 02:00:00       6    94.64              5  \n",
      "2018-01-01 03:00:00       5    80.61              7  \n",
      "2018-01-01 04:00:00      11   184.34              5  \n",
      " \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 43824 entries, 2018-01-01 00:00:00 to 2022-12-31 23:00:00\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Year           43824 non-null  int64  \n",
      " 1   Month          43824 non-null  int64  \n",
      " 2   Season         43824 non-null  object \n",
      " 3   DayOfWeek      43824 non-null  int64  \n",
      " 4   WeekDay        43824 non-null  object \n",
      " 5   Hour           43824 non-null  int64  \n",
      " 6   Holiday        43824 non-null  object \n",
      " 7   Weather        43824 non-null  object \n",
      " 8   SpecialEvent   120 non-null    object \n",
      " 9   CustomerCount  43824 non-null  int64  \n",
      " 10  Orders         43824 non-null  int64  \n",
      " 11  Revenue        43824 non-null  float64\n",
      " 12  StaffingLevel  43824 non-null  int64  \n",
      "dtypes: float64(1), int64(7), object(5)\n",
      "memory usage: 4.7+ MB\n",
      "None\n",
      "Missing data in each column:\n",
      "Year             False\n",
      "Month            False\n",
      "Season           False\n",
      "DayOfWeek        False\n",
      "WeekDay          False\n",
      "Hour             False\n",
      "Holiday          False\n",
      "Weather          False\n",
      "SpecialEvent      True\n",
      "CustomerCount    False\n",
      "Orders           False\n",
      "Revenue          False\n",
      "StaffingLevel    False\n",
      "dtype: bool\n",
      " \n",
      "\n",
      "Summary Statistics:\n",
      "               Year         Month     DayOfWeek          Hour  CustomerCount  \\\n",
      "count  43824.000000  43824.000000  43824.000000  43824.000000   43824.000000   \n",
      "mean    2020.000000      6.523549      3.998357     11.500000      19.081029   \n",
      "std        1.413842      3.448572      1.999337      6.922266      15.447294   \n",
      "min     2018.000000      1.000000      1.000000      0.000000       0.000000   \n",
      "25%     2019.000000      4.000000      2.000000      5.750000       7.000000   \n",
      "50%     2020.000000      7.000000      4.000000     11.500000      14.000000   \n",
      "75%     2021.000000     10.000000      6.000000     17.250000      31.000000   \n",
      "max     2022.000000     12.000000      7.000000     23.000000      77.000000   \n",
      "\n",
      "             Orders       Revenue  StaffingLevel  \n",
      "count  43824.000000  43824.000000   43824.000000  \n",
      "mean      13.445692    202.368099       5.928852  \n",
      "std       10.886505    163.660999       1.579194  \n",
      "min        0.000000      0.000000       3.000000  \n",
      "25%        5.000000     70.710000       5.000000  \n",
      "50%       10.000000    154.580000       6.000000  \n",
      "75%       22.000000    326.020000       7.000000  \n",
      "max       55.000000    850.630000      12.000000  \n",
      "Season\n",
      "Spring    11040\n",
      "Summer    11040\n",
      "Fall      10920\n",
      "Winter    10824\n",
      "Name: count, dtype: int64\n",
      "2018-01-01    24\n",
      "2018-01-02    24\n",
      "2018-01-03    24\n",
      "2018-01-04    24\n",
      "2018-01-05    24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess\n",
    "df = load_data('data/RestaurantData.csv')\n",
    "df_clean = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "restaurant_train, restaurant_test = split_train_test(df_clean, split_date='2022-01-01')\n",
    "# Format for SARIMAX\n",
    "train_series, test_series = prepare_sarimax_data(restaurant_train, restaurant_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMAX Exogenous Variables model refinement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the datetime index is correctly set and frequency is hourly\n",
    "restaurant_subset_train = restaurant_train.copy()\n",
    "restaurant_subset_test = restaurant_test.copy()\n",
    "\n",
    "restaurant_subset_train.index = pd.to_datetime(restaurant_subset_train.index)\n",
    "restaurant_subset_train = restaurant_subset_train.asfreq('h')\n",
    "\n",
    "restaurant_subset_test.index = pd.to_datetime(restaurant_subset_test.index)\n",
    "restaurant_subset_test = restaurant_subset_test.asfreq('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract exogenous variables (hour + holidays) for train and test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m exog_train, exog_test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_exogenous_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestaurant_subset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestaurant_subset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/My_project2/models/model_sarimax.py:384\u001b[0m, in \u001b[0;36mcreate_exogenous_variables\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m    381\u001b[0m hour_train \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mhour\u001b[38;5;241m.\u001b[39mto_frame(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    382\u001b[0m hour_test \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mhour\u001b[38;5;241m.\u001b[39mto_frame(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 384\u001b[0m exog_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhour_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholiday_dummies_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m exog_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([hour_test, holiday_dummies_test], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exog_train, exog_test\n",
      "File \u001b[0;32m~/My_project2/.venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/My_project2/.venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[0;32m--> 680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m \u001b[43mobj_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m    684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    686\u001b[0m )\n",
      "File \u001b[0;32m~/My_project2/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3885\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 3885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[1;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "# Extract exogenous variables (hour + holidays) for train and test\n",
    "exog_train, exog_test = create_exogenous_variables(restaurant_subset_train, restaurant_subset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
